        # decoder_matrix = self.ae.decoder

        # decoder_normed = decoder_matrix / decoder_matrix.norm(dim=1, keepdim=True)

        # sim_matrix = decoder_normed @ decoder_normed.T

        # sim_matrix.fill_diagonal_(-float("inf"))

        # max_sim = sim_matrix.max(dim=1).values

        # sum_max_sim = max_sim.sum()

        # decoder_matrix = self.ae.decoder
        # expert_dict_size = self.ae.expert_dict_size
        # experts = self.ae.experts

        # expert_centers = []
        # for expert in range(experts):
        #     start = expert * expert_dict_size
        #     end = (expert + 1) * expert_dict_size
        #     expert_features = decoder_matrix[start:end]
        #     center = expert_features.mean(dim=0, keepdim=True)
        #     expert_centers.append(center)
        # expert_centers = t.cat(expert_centers, dim=0)

        # centers_normed = expert_centers / expert_centers.norm(dim=1, keepdim=True)
        # sim_matrix = centers_normed @ centers_normed.T
        # sim_matrix.fill_diagonal_(float('-inf'))
        # mask = ~t.eye(experts, dtype=bool, device=sim_matrix.device)
        # sim_ext = sim_matrix[mask].mean()

        # sim_ints = []
        # for expert in range(experts):
        #     start = expert * expert_dict_size
        #     end = (expert + 1) * expert_dict_size
        #     expert_features = decoder_matrix[start:end]
        #     center = expert_centers[expert:expert+1]
        #     features_normed = expert_features / expert_features.norm(dim=1, keepdim=True)
        #     center_normed = center / center.norm(dim=1, keepdim=True)
        #     sim = (features_normed * center_normed).sum(dim=1).mean()
        #     sim_ints.append(sim)
        # sim_int = t.stack(sim_ints).mean()
        # sim_loss = (sim_ext + sim_int * 0.3) * expert_dict_size




    # try:
    #     import matplotlib.pyplot as plt

    #     # Determine number of experts and features per expert
    #     experts = dictionary.experts
    #     features_per_expert = decoder_matrix.shape[0] // experts

    #     # Compute t-SNE projection
    #     tsne = TSNE(n_components=2, random_state=0, init="pca", learning_rate="auto")
    #     decoder_proj = tsne.fit_transform(decoder_matrix.detach().cpu().numpy())

    #     # Assign colors by expert
    #     colors = plt.cm.get_cmap("tab20", experts)
    #     expert_ids = np.repeat(np.arange(experts), features_per_expert)

    #     plt.figure(figsize=(6, 6))
    #     for i in range(experts):
    #         idx = expert_ids == i
    #         plt.scatter(
    #             decoder_proj[idx, 0],
    #             decoder_proj[idx, 1],
    #             s=10,
    #             color=colors(i),
    #             label=f"Expert {i+1}",
    #             alpha=0.7,
    #         )
    #     plt.title("t-SNE projection of decoder features")
    #     plt.legend(markerscale=2, bbox_to_anchor=(1.05, 1), loc="upper left")
    #     plt.tight_layout()
    #     plt.savefig("decoder_tsne.png", dpi=300)
    #     plt.close()
    # except ImportError:
    #     if DEBUG:
    #         print("sklearn or matplotlib not installed, skipping t-SNE plot.")




    json_path = "/home/xuzhen/switch_sae/sae-auto-interp/result/MoE_SAE_Physically/similar_keywords_report.json"
    average_similarities, keyword_pairwise_sims_for_plotting = calculate_keyword_average_similarity(
        json_path,
        sim_matrix
    )
    if average_similarities:
        print("\n计算结果:")
        for keyword, avg_sim in average_similarities.items():
            print(f"关键词 '{keyword}': 平均相似度 = {avg_sim:.4f}")
    else:
        print("\n没有计算出任何关键词的平均相似度。")
    plot_output_dir = "similarity_plots"
    print(f"\n--- 正在生成相似度分布图到目录 '{plot_output_dir}' ---")
    os.makedirs(plot_output_dir, exist_ok=True)
    plot_overall_similarity_distribution(
        sim_matrix,
        save_path=os.path.join(plot_output_dir, 'overall_similarity_distribution.png')
    )
    plot_combined_keywords_similarity_distribution(
        keyword_pairwise_sims_for_plotting,
        save_path=os.path.join(plot_output_dir, 'keywords_similarity_distribution.png')
    )

    return out
import torch as t
import json
import numpy as np
import os
import matplotlib.pyplot as plt
from typing import Dict, List
from collections import defaultdict
from typing import Tuple
def calculate_keyword_average_similarity(
    json_filepath: str,
    sim_matrix: t.Tensor
) -> Tuple[Dict[str, float], Dict[str, List[float]]]:
    if not os.path.exists(json_filepath):
        print(f"错误: JSON 文件 '{json_filepath}' 不存在。")
        return {}, defaultdict(list)

    try:
        with open(json_filepath, 'r', encoding='utf-8') as f:
            keyword_data = json.load(f)
    except json.JSONDecodeError:
        print(f"错误: 无法解析 JSON 文件 '{json_filepath}'。请确保它是有效的 JSON 格式。")
        return {}, defaultdict(list)
    except Exception as e:
        print(f"读取 JSON 文件 '{json_filepath}' 时发生错误: {e}")
        return {}, defaultdict(list)

    if not isinstance(keyword_data, list):
        print("警告: JSON 文件的根元素不是列表。预期格式为 [{\"keyword\": \"...\", \"features\": [...]}, ...]。")
        return {}, defaultdict(list)

    results: Dict[str, float] = {}
    keyword_pairwise_sims: Dict[str, List[float]] = defaultdict(list)

    for entry in keyword_data:
        if not isinstance(entry, dict) or "keyword" not in entry or "features" not in entry:
            print(f"警告: JSON 条目格式不正确，跳过: {entry}")
            continue

        keyword = entry["keyword"]
        feature_ids = entry["features"]

        if not isinstance(feature_ids, list) or not all(isinstance(f_id, int) for f_id in feature_ids):
            print(f"警告: 关键词 '{keyword}' 的 features 列表格式不正确，跳过。")
            continue

        if len(feature_ids) < 2:
            # 单个或没有 feature ID 的关键词无法计算两两相似度，跳过。
            continue
        
        pairwise_similarities_for_current_keyword = []
        for i in range(len(feature_ids)):
            for j in range(i + 1, len(feature_ids)): # 从 i+1 开始，避免重复和对角线
                id1 = feature_ids[i]
                id2 = feature_ids[j]

                # 检查 feature ID 是否在 sim_matrix 的有效索引范围内
                if id1 < sim_matrix.shape[0] and id2 < sim_matrix.shape[1] and id1 >= 0 and id2 >= 0:
                    similarity = sim_matrix[id1, id2].item()
                    pairwise_similarities_for_current_keyword.append(similarity)
                else:
                    print(f"警告: 关键词 '{keyword}' 的 feature ID ({id1} 或 {id2}) 超出 sim_matrix 的有效范围 ({sim_matrix.shape[0]}x{sim_matrix.shape[1]})。跳过此对。")

        if pairwise_similarities_for_current_keyword:
            average_similarity = np.mean(pairwise_similarities_for_current_keyword)
            results[keyword] = average_similarity
            # 将当前关键词的所有两两相似度添加到集合中，用于后续的合并绘图
            keyword_pairwise_sims[keyword].extend(pairwise_similarities_for_current_keyword)
        else:
            print(f"警告: 关键词 '{keyword}' 没有有效或可计算的两两相似度。")

    return results, keyword_pairwise_sims

# --- 绘图函数 ---

def plot_overall_similarity_distribution(sim_matrix: t.Tensor, save_path: str = None):
    sim_matrix_np = sim_matrix.cpu().numpy()
    non_diagonal_sims = []
    num_features = sim_matrix_np.shape[0]
    for i in range(num_features):
        for j in range(i + 1, num_features):
            non_diagonal_sims.append(sim_matrix_np[i, j])
    
    if not non_diagonal_sims:
        print("警告: sim_matrix 中没有非对角线相似度值可供绘制。")
        return
        
    non_diagonal_sims = np.array(non_diagonal_sims)

    plt.figure(figsize=(10, 6))
    plt.hist(non_diagonal_sims, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')
    plt.xlabel('相似度值 (余弦相似度)')
    plt.ylabel('密度')
    plt.grid(axis='y', alpha=0.75)
    plt.xlim([-1, 1])

    if save_path:
        plt.savefig(save_path)
        print(f"整体相似度分布图已保存到: {save_path}")
    else:
        plt.show()
    plt.close()

def plot_combined_keywords_similarity_distribution(
    keyword_pairwise_sims: Dict[str, List[float]],
    save_path: str = None
):
    all_combined_similarities = []
    
    for keyword, similarities in keyword_pairwise_sims.items():
        if similarities:
            all_combined_similarities.extend(similarities)
            
    if not all_combined_similarities:
        print("警告: 没有收集到任何关键词相关的相似度数据可供绘制合并图。")
        return

    all_combined_similarities_np = np.array(all_combined_similarities)

    plt.figure(figsize=(12, 7))
    plt.hist(all_combined_similarities_np, bins=50, density=True, alpha=0.7,
             color='forestgreen', edgecolor='black')
    
    plt.title('所有关键词关联特征的合并相似度分布')
    plt.xlabel('相似度值 (余弦相似度)')
    plt.ylabel('密度')
    plt.grid(axis='y', alpha=0.75)
    plt.xlim([-1, 1])
    
    mean_sim = np.mean(all_combined_similarities_np)
    median_sim = np.median(all_combined_similarities_np)
    plt.axvline(mean_sim, color='red', linestyle='dashed', linewidth=1, label=f'平均值: {mean_sim:.2f}')
    plt.axvline(median_sim, color='purple', linestyle='dashed', linewidth=1, label=f'中位数: {median_sim:.2f}')
    plt.legend()

    if save_path:
        plt.savefig(save_path)
        print(f"所有关键词合并相似度分布图已保存到: {save_path}")
    else:
        plt.show()
    plt.close()

    out = {}
    out["mean_decoder_max_similarity"] = mean_max_sim
    top_k_to_print = 100
    if sim_matrix.numel() == 0 or (sim_matrix == -float("inf")).all():
        print("未找到有效相似度对。")
    else:
        flat_sim_matrix = sim_matrix.flatten()
        top_k_values, top_k_flat_indices = t.topk(flat_sim_matrix, k=top_k_to_print)

        print(f"\n--- sim_matrix 中最大的 {top_k_to_print} 个相似度及其对应的特征 ---")
        num_rows = sim_matrix.shape[0]
        for i in range(top_k_to_print):
            sim_val = top_k_values[i].item()
            flat_idx = top_k_flat_indices[i].item()

            row_idx = flat_idx // num_rows
            col_idx = flat_idx % num_rows
            if sim_val == -float("inf"):
                print(f"警告: 仅找到 {i} 对有效相似度。")
                break
            print(f"相似度: {sim_val:.4f}, 特征1索引: {row_idx}, 特征2索引: {col_idx}")

        with t.no_grad():
            gate = self.ae.gate(x - self.ae.b_gate)
            _, top_indices = gate.topk(self.ae.e, dim=-1)
            activated_experts = top_indices.unique().tolist()
            # mask for decoder
            mask_dec = t.zeros_like(self.ae.decoder.grad)
            # mask for encoder.weight
            mask_enc = t.zeros_like(self.ae.encoder.weight.grad)
            for expert in activated_experts:
                start = expert * self.ae.expert_dict_size
                end = (expert + 1) * self.ae.expert_dict_size
                mask_dec[start:end, :] = 1
                mask_enc[start:end, :] = 1
            self.ae.decoder.grad *= mask_dec
            self.ae.encoder.weight.grad *= mask_enc

    num_experts = dictionary.experts
    total_dict_size = dictionary.dict_size

    if num_experts > 0 and total_dict_size > 0:
        dict_size_per_expert = total_dict_size // num_experts
        if total_dict_size % num_experts != 0:
            print(f"警告：总特征数 {total_dict_size} 不能被专家数量 {num_experts} 整除。这可能导致专家宽度不均。")

        intra_expert_sims = []
        for i in range(num_experts):
            start_idx = i * dict_size_per_expert
            end_idx = start_idx + dict_size_per_expert
            sim_intra_expert = sim_matrix[start_idx:end_idx, start_idx:end_idx]
            
            mask = ~t.eye(sim_intra_expert.shape[0], dtype=bool, device=sim_intra_expert.device)
            valid_sims = sim_intra_expert[mask]
            if valid_sims.numel() > 0:
                intra_expert_sims.extend(valid_sims.tolist())
        
        if intra_expert_sims:
            out["mean_intra_expert_decoder_similarity"] = float(np.mean(intra_expert_sims))
        else:
            out["mean_intra_expert_decoder_similarity"] = 0.0

        all_inter_expert_similarities = []

        if num_experts > 1:
            for i in range(num_experts):
                start_idx_i = i * dict_size_per_expert
                end_idx_i = start_idx_i + dict_size_per_expert
                
                for j in range(num_experts):
                    if i == j:
                        continue
                    
                    start_idx_j = j * dict_size_per_expert
                    end_idx_j = start_idx_j + dict_size_per_expert
                    
                    sim_i_to_j = sim_matrix[start_idx_i:end_idx_i, start_idx_j:end_idx_j]
                    
                    all_inter_expert_similarities.extend(sim_i_to_j.flatten().tolist())

            if all_inter_expert_similarities:
                out["mean_inter_expert_decoder_similarity"] = float(np.mean(all_inter_expert_similarities))
            else:
                out["mean_inter_expert_decoder_similarity"] = 0.0
        else:
            out["mean_inter_expert_decoder_similarity"] = 0.0
    else:
        out["mean_intra_expert_decoder_similarity"] = 0.0
        out["mean_inter_expert_decoder_similarity"] = 0.0