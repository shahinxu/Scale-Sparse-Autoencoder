import torch as t
from nnsight import LanguageModel
from dictionary_learning.test_buffer import ActivationBuffer
from dictionary_learning.trainers.moe_physically import MultiExpertAutoEncoder
import json
from transformers import AutoTokenizer
from config import lm, activation_dim, layer, n_ctxs
from collections import defaultdict
import os
import pandas as pd

GPU = "5"
MODEL = "MultiExpert_64_8"
MODEL_PATH = f"/home/xuzhen/switch_sae/dictionaries/{MODEL}/2.pt"
OUTPUT_ROOT = f"expert_feature_analysis_{MODEL}_wikitext"

WIKITEXT_PATH = "/home/xuzhen/switch_sae/wikitext"
WIKITEXT_VERSION = "wikitext-2-raw-v1"
SPLIT = "train"

BATCH_SIZE = 200
TOTAL_BATCHES = 10
TARGET_EXPERTS = [0, 1]


def load_wikitext_batch(wikitext_path, version="wikitext-2-raw-v1", split="train", 
                       batch_size=200, batch_idx=0, min_length=20, max_length=200):
    """æ‰¹æ¬¡åŠ è½½WikiTextæ•°æ®é›†"""
    
    dataset_path = os.path.join(wikitext_path, version)
    parquet_files = []
    
    if os.path.exists(dataset_path):
        all_files = os.listdir(dataset_path)
        parquet_files = [f for f in all_files if f.startswith(f"{split}-") and f.endswith(".parquet")]
        parquet_files.sort()
    
    if not parquet_files:
        raise FileNotFoundError(f"No parquet files found for {split} split in {dataset_path}")
    
    print(f"Loading batch {batch_idx} (size: {batch_size})")
    
    all_texts = []
    texts_read = 0
    start_idx = batch_idx * batch_size
    end_idx = start_idx + batch_size
    
    for parquet_file in parquet_files:
        full_path = os.path.join(dataset_path, parquet_file)
        df = pd.read_parquet(full_path)
        
        for _, row in df.iterrows():
            text = row['text'].strip()
            
            if len(text) < min_length:
                continue
            if len(text) > max_length:
                text = text[:max_length]
            if text.startswith('=') and text.endswith('='):
                continue
            if not text or text.isspace():
                continue
            if len(text.split()) < 3:
                continue
            
            if texts_read < start_idx:
                texts_read += 1
                continue
                
            if texts_read < end_idx:
                all_texts.append(text)
                texts_read += 1
            else:
                break
        
        if len(all_texts) >= batch_size:
            break
    
    print(f"  Loaded {len(all_texts)} texts for batch {batch_idx}")
    return all_texts


class ExpertFeatureCollector:
    """æ”¶é›†æŒ‡å®šexpertçš„æ¯ä¸ªfeatureçš„æ¿€æ´»tokenä¿¡æ¯"""
    
    def __init__(self, target_experts, expert_dict_size=768*32):
        self.target_experts = set(target_experts)
        self.expert_dict_size = expert_dict_size
        
        # expert_id -> feature_id -> list of token activations
        self.expert_feature_tokens = defaultdict(lambda: defaultdict(list))
        
        # expert_id -> feature_id -> max activation strength
        self.expert_feature_max_strength = defaultdict(lambda: defaultdict(float))
        
        # expert_id -> feature_id -> best example
        self.expert_feature_best_example = defaultdict(dict)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.expert_stats = defaultdict(lambda: {
            'total_features_activated': 0,
            'total_token_activations': 0,
            'texts_processed': 0
        })
        
        self.total_texts_processed = 0
        self.total_batches_processed = 0
    
    def add_feature_activation(self, expert_id, feature_id, token_text, activation_strength, 
                             text_id, token_pos, original_text):
        """æ·»åŠ ä¸€ä¸ªfeatureçš„tokenæ¿€æ´»è®°å½•"""
        
        if expert_id not in self.target_experts:
            return
        
        global_text_id = self.total_texts_processed + text_id
        
        # è®¡ç®—ç›¸å¯¹feature ID (åœ¨è¯¥expertå†…çš„ID)
        relative_feature_id = feature_id % self.expert_dict_size
        
        # æ·»åŠ tokenæ¿€æ´»è®°å½•
        token_record = {
            'token': token_text,
            'strength': activation_strength,
            'text_id': global_text_id,
            'token_pos': token_pos,
            'original_text': original_text[:100] + '...' if len(original_text) > 100 else original_text
        }
        
        self.expert_feature_tokens[expert_id][relative_feature_id].append(token_record)
        
        # æ›´æ–°æœ€å¤§æ¿€æ´»å¼ºåº¦å’Œæœ€ä½³ç¤ºä¾‹
        if activation_strength > self.expert_feature_max_strength[expert_id][relative_feature_id]:
            self.expert_feature_max_strength[expert_id][relative_feature_id] = activation_strength
            self.expert_feature_best_example[expert_id][relative_feature_id] = token_record
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.expert_stats[expert_id]['total_token_activations'] += 1
        
        # æ›´æ–°featureè®¡æ•°ï¼ˆåªè®¡ç®—å·²æ¿€æ´»çš„featureï¼‰
        self.expert_stats[expert_id]['total_features_activated'] = len(self.expert_feature_tokens[expert_id])
    
    def update_batch_stats(self, batch_size):
        """æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯"""
        self.total_texts_processed += batch_size
        self.total_batches_processed += 1
        
        for expert_id in self.target_experts:
            self.expert_stats[expert_id]['texts_processed'] = self.total_texts_processed
    
    def get_expert_feature_summary(self, expert_id, top_n=20):
        """è·å–æŒ‡å®šexpertçš„featureæ‘˜è¦"""
        if expert_id not in self.expert_feature_tokens:
            return None
        
        features_data = []
        
        for feature_id, token_records in self.expert_feature_tokens[expert_id].items():
            max_strength = self.expert_feature_max_strength[expert_id][feature_id]
            best_example = self.expert_feature_best_example[expert_id][feature_id]
            
            # ç»Ÿè®¡unique tokens
            unique_tokens = set(record['token'] for record in token_records)
            
            # è·å–æœ€å¼ºæ¿€æ´»çš„å‰å‡ ä¸ªtoken
            sorted_records = sorted(token_records, key=lambda x: x['strength'], reverse=True)
            top_tokens = sorted_records[:top_n]
            
            features_data.append({
                'feature_id': feature_id,
                'max_activation': max_strength,
                'total_activations': len(token_records),
                'unique_tokens_count': len(unique_tokens),
                'unique_tokens': list(unique_tokens),
                'top_activations': top_tokens,
                'best_example': best_example
            })
        
        # æŒ‰æœ€å¤§æ¿€æ´»å¼ºåº¦æ’åº
        features_data.sort(key=lambda x: x['max_activation'], reverse=True)
        
        return {
            'expert_id': expert_id,
            'statistics': self.expert_stats[expert_id],
            'total_features': len(features_data),
            'features': features_data
        }
    
    def save_expert_feature_analysis(self, output_dir):
        """ä¿å­˜expert featureåˆ†æç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # å…¨å±€ç»Ÿè®¡
        global_stats = {
            'target_experts': list(self.target_experts),
            'total_texts_processed': self.total_texts_processed,
            'total_batches_processed': self.total_batches_processed,
            'expert_feature_counts': {
                expert_id: len(self.expert_feature_tokens[expert_id]) 
                for expert_id in self.target_experts
            },
            'dataset_info': {
                'source': 'WikiText',
                'version': WIKITEXT_VERSION,
                'split': SPLIT,
                'batch_size': BATCH_SIZE,
                'total_batches': TOTAL_BATCHES
            }
        }
        
        with open(os.path.join(output_dir, 'global_statistics.json'), 'w', encoding='utf-8') as f:
            json.dump(global_stats, f, indent=2, ensure_ascii=False)
        
        # ä¸ºæ¯ä¸ªtarget expertç”Ÿæˆè¯¦ç»†åˆ†æ
        for expert_id in self.target_experts:
            if expert_id not in self.expert_feature_tokens:
                continue
                
            expert_dir = os.path.join(output_dir, f'expert_{expert_id:02d}')
            os.makedirs(expert_dir, exist_ok=True)
            
            # è·å–å®Œæ•´çš„featureæ‘˜è¦
            summary = self.get_expert_feature_summary(expert_id, top_n=50)
            
            # ä¿å­˜å®Œæ•´çš„JSONåˆ†æ
            with open(os.path.join(expert_dir, 'feature_analysis.json'), 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜å¯è¯»çš„æ–‡æœ¬æŠ¥å‘Š
            self._save_readable_feature_report(expert_dir, summary)
            
            # ä¿å­˜æ¯ä¸ªfeatureçš„è¯¦ç»†tokenåˆ—è¡¨
            self._save_feature_token_details(expert_dir, expert_id)
    
    def _save_readable_feature_report(self, expert_dir, summary):
        """ä¿å­˜å¯è¯»çš„featureæŠ¥å‘Š"""
        with open(os.path.join(expert_dir, 'feature_report.txt'), 'w', encoding='utf-8') as f:
            expert_id = summary['expert_id']
            stats = summary['statistics']
            
            f.write(f"Expert {expert_id} - Feature Analysis Report (WikiText)\n")
            f.write("="*70 + "\n\n")
            
            f.write("ğŸ“Š Statistics:\n")
            f.write(f"  Total Features Activated: {stats['total_features_activated']}\n")
            f.write(f"  Total Token Activations: {stats['total_token_activations']}\n")
            f.write(f"  Texts Processed: {stats['texts_processed']}\n")
            if stats['total_features_activated'] > 0:
                f.write(f"  Avg Activations per Feature: {stats['total_token_activations']/stats['total_features_activated']:.2f}\n\n")
            
            f.write("ğŸ”¥ Top Features by Maximum Activation:\n")
            f.write("-"*70 + "\n")
            
            for i, feature_data in enumerate(summary['features'][:20], 1):
                feature_id = feature_data['feature_id']
                max_activation = feature_data['max_activation']
                total_activations = feature_data['total_activations']
                unique_tokens = feature_data['unique_tokens_count']
                best_example = feature_data['best_example']
                
                f.write(f"\n{i:2d}. Feature {feature_id:4d}:\n")
                f.write(f"    Max Activation: {max_activation:.4f}\n")
                f.write(f"    Total Activations: {total_activations}\n")
                f.write(f"    Unique Tokens: {unique_tokens}\n")
                f.write(f"    Best Token: '{best_example['token']}' (strength: {best_example['strength']:.4f})\n")
                f.write(f"    Context: {best_example['original_text']}\n")
                
                # æ˜¾ç¤ºè¯¥featureçš„å‰10ä¸ªæœ€å¼ºtoken
                top_tokens = feature_data['top_activations'][:10]
                token_list = [f"'{record['token']}'({record['strength']:.3f})" for record in top_tokens]
                f.write(f"    Top Tokens: {', '.join(token_list)}\n")
    
    def _save_feature_token_details(self, expert_dir, expert_id):
        """ä¿å­˜æ¯ä¸ªfeatureçš„è¯¦ç»†tokenä¿¡æ¯"""
        features_dir = os.path.join(expert_dir, 'features')
        os.makedirs(features_dir, exist_ok=True)
        
        for feature_id, token_records in self.expert_feature_tokens[expert_id].items():
            feature_file = os.path.join(features_dir, f'feature_{feature_id:04d}.txt')
            
            with open(feature_file, 'w', encoding='utf-8') as f:
                f.write(f"Expert {expert_id} - Feature {feature_id} - Token Activations\n")
                f.write("="*60 + "\n\n")
                
                # æŒ‰æ¿€æ´»å¼ºåº¦æ’åº
                sorted_records = sorted(token_records, key=lambda x: x['strength'], reverse=True)
                
                f.write(f"Total Activations: {len(token_records)}\n")
                f.write(f"Unique Tokens: {len(set(record['token'] for record in token_records))}\n")
                f.write(f"Max Activation: {max(record['strength'] for record in token_records):.4f}\n\n")
                
                f.write("All Token Activations (sorted by strength):\n")
                f.write("-"*60 + "\n")
                
                for i, record in enumerate(sorted_records, 1):
                    f.write(f"{i:3d}. '{record['token']}' ")
                    f.write(f"(strength: {record['strength']:.4f}, ")
                    f.write(f"text_{record['text_id']}, pos_{record['token_pos']})\n")
                    f.write(f"     Context: {record['original_text']}\n\n")


@t.no_grad()
def analyze_batch(dictionary, model, submodule, device, texts, batch_idx, collector):
    """åˆ†æä¸€ä¸ªæ‰¹æ¬¡çš„æ–‡æœ¬ï¼Œæ”¶é›†target expertsçš„featureæ¿€æ´»ä¿¡æ¯"""
    
    print(f"\n{'='*60}")
    print(f"Processing Batch {batch_idx + 1}/{TOTAL_BATCHES}")
    print(f"Batch size: {len(texts)} texts")
    print(f"Target experts: {TARGET_EXPERTS}")
    print(f"{'='*60}")
    
    tokenizer = AutoTokenizer.from_pretrained(lm)
    
    def gen():
        while True:
            for text in texts:
                input_ids = tokenizer.encode(text, truncation=True, max_length=128)
                processed_text = tokenizer.decode(input_ids, skip_special_tokens=True)
                yield processed_text
    
    buffer = ActivationBuffer(
        gen(), 
        model, 
        submodule, 
        d_submodule=activation_dim, 
        n_ctxs=min(n_ctxs, len(texts) * 150),
        device=device,
        sequential=True
    )
    
    batch_feature_activations = defaultdict(int)
    
    for text_id, text in enumerate(texts):
        try:
            x = next(buffer).to(device)
        except StopIteration:
            print(f"Warning: Not enough activations for text {text_id}")
            break
        
        input_ids = tokenizer.encode(text, truncation=True, max_length=128)
        tokens = [tokenizer.decode([token_id]) for token_id in input_ids]
        
        if text_id % 50 == 0:
            print(f"  Processing text {text_id}/{len(texts)}: '{text[:50]}...'")
        
        for token_pos in range(min(len(x), len(tokens))):
            token_activation = x[token_pos]
            token_text = tokens[token_pos]
            
            # è·å–SAEçš„featureæ¿€æ´»
            _, f = dictionary(token_activation.unsqueeze(0), output_features=True)
            token_features = f[0]
            
            # è·å–top-kæ¿€æ´»çš„features
            top_k_values, top_k_indices = token_features.topk(dictionary.k, sorted=True)
            expert_dict_size = dictionary.expert_dict_size
            
            for fid, fval in zip(top_k_indices, top_k_values):
                if fval.item() > 0:
                    expert_id = fid.item() // expert_dict_size
                    
                    # åªå¤„ç†target experts
                    if expert_id in TARGET_EXPERTS:
                        collector.add_feature_activation(
                            expert_id=expert_id,
                            feature_id=fid.item(),
                            token_text=token_text,
                            activation_strength=fval.item(),
                            text_id=text_id,
                            token_pos=token_pos,
                            original_text=text
                        )
                        batch_feature_activations[expert_id] += 1
    
    collector.update_batch_stats(len(texts))
    
    print(f"  Batch {batch_idx + 1} completed:")
    for expert_id in TARGET_EXPERTS:
        activations = batch_feature_activations.get(expert_id, 0)
        total_features = len(collector.expert_feature_tokens[expert_id])
        print(f"    Expert {expert_id}: {activations} activations, {total_features} features")
    
    del buffer
    t.cuda.empty_cache()


def main():
    device = f'cuda:{GPU}'
    
    print(f"Expert Feature Analysis Configuration:")
    print(f"  Dataset: {WIKITEXT_VERSION}")
    print(f"  Split: {SPLIT}")
    print(f"  Model: {MODEL}")
    print(f"  Device: {device}")
    print(f"  Target Experts: {TARGET_EXPERTS}")
    print(f"  Batch processing: {BATCH_SIZE} texts per batch, {TOTAL_BATCHES} batches")
    print(f"  Total texts to process: {BATCH_SIZE * TOTAL_BATCHES}")
    
    print("\nLoading language model...")
    model = LanguageModel(lm, dispatch=True, device_map=device)
    submodule = model.transformer.h[layer]
    
    print(f"Loading SAE from {MODEL_PATH}...")
    ae = MultiExpertAutoEncoder(
        activation_dim=768, 
        dict_size=32*768, 
        k=4, 
        experts=64, 
        e=8, 
        heaviside=False
    )
    ae.load_state_dict(t.load(MODEL_PATH))
    ae.to(device)
    ae.eval()
    
    # åˆå§‹åŒ–featureæ”¶é›†å™¨
    collector = ExpertFeatureCollector(
        target_experts=TARGET_EXPERTS,
        expert_dict_size=ae.expert_dict_size
    )
    
    # æ‰¹æ¬¡å¤„ç†
    for batch_idx in range(TOTAL_BATCHES):
        try:
            # åŠ è½½å½“å‰æ‰¹æ¬¡çš„æ•°æ®
            batch_texts = load_wikitext_batch(
                wikitext_path=WIKITEXT_PATH,
                version=WIKITEXT_VERSION,
                split=SPLIT,
                batch_size=BATCH_SIZE,
                batch_idx=batch_idx
            )
            
            if not batch_texts:
                print(f"No more texts available at batch {batch_idx}")
                break
            
            # åˆ†æå½“å‰æ‰¹æ¬¡
            analyze_batch(ae, model, submodule, device, batch_texts, batch_idx, collector)
            
            # æ¯å¤„ç†å‡ ä¸ªæ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if (batch_idx + 1) % 2 == 0:
                print(f"\nSaving intermediate results after batch {batch_idx + 1}...")
                collector.save_expert_feature_analysis(OUTPUT_ROOT)
            
        except Exception as e:
            print(f"Error processing batch {batch_idx}: {e}")
            continue
    
    # æœ€ç»ˆä¿å­˜
    print(f"\nSaving final expert feature analysis results...")
    collector.save_expert_feature_analysis(OUTPUT_ROOT)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nâœ… Expert Feature Analysis Complete!")
    print(f"ğŸ“Š Final Statistics:")
    print(f"  Total texts processed: {collector.total_texts_processed}")
    print(f"  Total batches processed: {collector.total_batches_processed}")
    
    for expert_id in TARGET_EXPERTS:
        if expert_id in collector.expert_feature_tokens:
            stats = collector.expert_stats[expert_id]
            feature_count = len(collector.expert_feature_tokens[expert_id])
            print(f"\n  Expert {expert_id}:")
            print(f"    Features activated: {feature_count}")
            print(f"    Total activations: {stats['total_token_activations']}")
            if feature_count > 0:
                print(f"    Avg activations per feature: {stats['total_token_activations']/feature_count:.2f}")
    
    print(f"\nğŸ“ Results saved to: {OUTPUT_ROOT}/")
    print(f"  - global_statistics.json: å…¨å±€ç»Ÿè®¡ä¿¡æ¯")
    print(f"  - expert_XX/feature_analysis.json: æ¯ä¸ªexpertçš„å®Œæ•´åˆ†æ")
    print(f"  - expert_XX/feature_report.txt: å¯è¯»çš„åˆ†ææŠ¥å‘Š")
    print(f"  - expert_XX/features/feature_XXXX.txt: æ¯ä¸ªfeatureçš„è¯¦ç»†tokenåˆ—è¡¨")


if __name__ == "__main__":
    main()