{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Features Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by pip installing the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd sae_auto_interp && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣ - Loading your own autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `nnsight` library to attach autoencoders to the module tree. \n",
    "\n",
    "At the time of writing (8/8/24), this feature isn't yet available on the main version of `nnsight`. Please install the `0.3` branch.\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/ndif-team/nnsight.git@0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we'll load, cache, and evaluate some layer zero features from the recent OpenAI topk autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from sae_auto_interp.autoencoders.wrapper import AutoencoderLatents\n",
    "from sae_auto_interp.autoencoders.OpenAI import Autoencoder\n",
    "\n",
    "path = \"/home/xuzhen/switch_sae/dictionaries/dict_class:_lb.MoEAutoEncoder'>_activation_dim:768_dict_size:24576_auxk_alpha:0.03125_decay_start:560_steps:700_seed:0_device:cuda:3_layer:8_lm_name:xuzhenswitch_saegpt2_wandb_name:MoEAutoEncoder_k:32_experts:64_e:2_heaviside:False/6.pt\" # Change this line to your weights location.\n",
    "state_dict = torch.load(path)\n",
    "ae = Autoencoder.from_state_dict(state_dict=state_dict)\n",
    "ae.to(\"cuda:0\")\n",
    "\n",
    "model = LanguageModel(\"/home/xuzhen/switch_sae/gpt2\", device_map=\"auto\", dispatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helpful wrapper for collecting autoencoder latents. The wrapper is a `torch.nn.Module` which calls a given `forward` method at every forward pass. We'll use `partial` here so we don't run into late binding issues. \n",
    "\n",
    "If we use a lambda like `lambda x: ae.encode(x)[0]`, our wrappers will get only get a refrence to the last autoencoder's `encode` method in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward(ae, x):\n",
    "    latents, _ = ae.encode(x)\n",
    "    return latents\n",
    "\n",
    "# We can simply add the new module as an attribute to an existing\n",
    "# submodule on GPT-2's module tree.\n",
    "submodule = model.transformer.h[0]\n",
    "submodule.ae = AutoencoderLatents(\n",
    "    ae, \n",
    "    partial(_forward, ae),\n",
    "    width=131_072\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use `nnsight`'s `edit` context to set default interventions on the model's forward pass. \n",
    "\n",
    "Check out the [official demo](https://github.com/ndif-team/nnsight/blob/main/NNsight_v0_2.ipynb) to learn more about `nnsight` (which will be updated to 0.3 soon).\n",
    "\n",
    "As a quick refresher, `nnsight` allows users to execute PyTorch models, with interventions, lazily. A context manager collects operations, then compiles and executes them on completion. The `.edit` context defines default nodes in the intervention graph to be compiled on execution of the real model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.edit(\" \"):\n",
    "    acts = submodule.output[0]\n",
    "    submodule.ae(acts, hook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now collecting latents is as simple as saving the output of the submodule within the trace. This is uniquely helpful because (a) we can just handle references to submodules and access their `.ae` property which (b) removes the complexity of having to store a dictionary of submodules and their respective autoencoders, then passing the submodule's activations through the autoencoder every forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(\"hello, my name is\"):\n",
    "    latents = submodule.ae.output.save()\n",
    "\n",
    "latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is a quite a bit of boilerplate, so we provide some starter code within the `.autoencoders` module. See the available options in the `__init__.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2️⃣ - Caching Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an edited model, lets cache activations for the first one hundred features in the autoencoder across 100k tokens. Ideally, you'll want to cache on as many tokens as necessary to get a wide distribution of activations for your autoencoder's rarer features.\n",
    "\n",
    "Let's define a couple of constants for our cache and load tokens. Again, we provide utils for loading a `torch.utils.data.Dataset` of tokens, but feel free to load and tokenize however you want. Note that our tokenizer appends padding to the start of every sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.features import FeatureCache\n",
    "from sae_auto_interp.utils import load_tokenized_data\n",
    "\n",
    "CTX_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "N_TOKENS = 500_000\n",
    "N_SPLITS = 2\n",
    "\n",
    "tokens = load_tokenized_data(\n",
    "    CTX_LEN,\n",
    "    model.tokenizer,\n",
    "    \"kh4dien/fineweb-100m-sample\",\n",
    "    \"train[:15%]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cache accepts two dictionaries. \n",
    "\n",
    "`submodule_dict` is a `Dict[str, nnsight.Envoy]` which is iterated through during caching. \n",
    "\n",
    "`module_filter` is an optional filter for which we mask feature_ids found from caching. Note that this process is a slower, especially for larger numbers of tokens. However, it's very helpful for conserving CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = submodule._module_path\n",
    "\n",
    "submodule_dict = {module_path : submodule}\n",
    "module_filter = {module_path : torch.arange(100).to(\"cuda:0\")}\n",
    "\n",
    "cache = FeatureCache(\n",
    "    model, \n",
    "    submodule_dict, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    filters=module_filter\n",
    ")\n",
    "\n",
    "cache.run(N_TOKENS, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw features are saved as `safetensors` with the structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"location\" : torch.Tensor[\"n_activations\", 3],\n",
    "    \"activations\" : torch.Tensor[\"n_activations\"],\n",
    "}\n",
    "```\n",
    "\n",
    "Where each row of locations points to an activation, with the data `[batch_idx, seq_pos, feature_id]`. We also provide a splits parameter to save splits of the features into different `safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = \"raw_features/gpt2_128k\" # Change this line to your save location.\n",
    "cache.save_splits(\n",
    "    n_splits=N_SPLITS,\n",
    "    save_dir=raw_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3️⃣ - Loading Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a data loader for reconstructing features from their locations and activations. \n",
    "\n",
    "The loader requires a `FeatureConfig` which details how features were saved and how to reconstruct examples. \n",
    "\n",
    "The `ExperimentConfig` configures how train and test examples are sampled for explanation and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.features import FeatureDataset, pool_max_activation_windows, sample\n",
    "from sae_auto_interp.config import FeatureConfig, ExperimentConfig\n",
    "\n",
    "cfg = FeatureConfig(\n",
    "    width = 131_072,\n",
    "    min_examples = 200,\n",
    "    max_examples = 10_000,\n",
    "    example_cfg_len = 20,\n",
    "    n_splits = 2\n",
    ")\n",
    "\n",
    "sample_cfg = ExperimentConfig()\n",
    "\n",
    "dataset = FeatureDataset(\n",
    "    raw_dir=raw_dir,\n",
    "    cfg=cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.load` method of dataset accepts functions to reconstruct and sample activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor=partial(\n",
    "    pool_max_activation_windows,\n",
    "    tokens=tokens,\n",
    "    ctx_len=sample_cfg.example_ctx_len,\n",
    "    max_examples=cfg.max_examples,\n",
    ")\n",
    "\n",
    "sampler = partial(\n",
    "    sample,\n",
    "    cfg=sample_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a batch of records! The `.load` method is an iterator that just returns all records in a split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for records in dataset.load(constructor=constructor, sampler=sampler):\n",
    "    break\n",
    "\n",
    "record = records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display method in `.utils` just renders examples as html with their activating tokens highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.utils import display\n",
    "\n",
    "print(record.feature)\n",
    "display(record, model.tokenizer, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3️⃣ - Explaining Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define several clients for querying completion APIs such as vLLM and OpenRouter. For this example, we'll just use the OpenRouter client with `gpt-4o-mini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.clients import VLLM\n",
    "\n",
    "client = VLLM('TheBloke/Mistral-7B-Instruct-v0.2-GPTQ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just load an explainer and pass the client, a tokenizer, and generation configs as optional keyword arguments. The explainer outputs an `ExplainerResult` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.explainers import SimpleExplainer\n",
    "\n",
    "explainer = SimpleExplainer(\n",
    "    client,\n",
    "    model.tokenizer,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "explainer_result = await explainer(record)\n",
    "\n",
    "print(explainer_result.explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4️⃣ - Explaining Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can score explanations by loading a scorer and passing an feature record. The record should be updated to contain the `.explanation` attribute. \n",
    "\n",
    "In this example, we use the `RecallScorer` which requires random, non-activating examples to measure precision. For simplicity, we didn't sample those earlier so we'll just set those to train examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.scorers import RecallScorer\n",
    "\n",
    "scorer = RecallScorer(\n",
    "    client,\n",
    "    model.tokenizer,\n",
    "    max_tokens=25,\n",
    "    temperature=0.0,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "record.explanation = explainer_result.explanation\n",
    "record.random_examples = record.train\n",
    "\n",
    "score = await scorer(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We got a score. The `.score` attribute contains a list of `ClassifierOutput`s. For each `ClassifierOutput`, we have the following attributes:\n",
    "\n",
    "- `distance` : The quantile of the sample.\n",
    "- `ground_truth` : Whether the sample actually activated or not.\n",
    "- `prediction` : The model's prediction for whether the example activated. \n",
    "- `highlighted` : Whether the example was \"highlighted\" or not. Only True for the `FuzzScorer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "switch_sae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
